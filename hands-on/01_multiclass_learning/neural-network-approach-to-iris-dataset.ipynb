{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらのノートブックはKaggleの以下のページを参考に作られています\n",
    "\n",
    "https://www.kaggle.com/code/louisong97/neural-network-approach-to-iris-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "097d5a77-5283-4d43-ac86-8e591645dee4",
    "_execution_state": "idle",
    "_uuid": "8c105b13ca3ab5034a98e7ab4dc34a844b518a4d",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 導入\n",
    "\n",
    "こちらのノートブックでは、**Neural Network（ニューラルネットワーク）**を用いて、花の**アヤメ（Iris）**の種類を分類するタスクに取り組みます。\n",
    "\n",
    "このタスクでは、機械学習における基本的な分類問題を題材に、ニューラルネットワークの構築・学習・評価の流れを実践的に学びます。\n",
    "\n",
    "## Neural Networkとは？\n",
    "\n",
    "ニューラルネットワークとは、人間の脳の神経細胞（ニューロン）の仕組みを模倣したモデルです。  \n",
    "入力層（Input Layer）、隠れ層（Hidden Layer）、出力層（Output Layer）の層構造からなり、データの特徴を段階的に抽出していきます。\n",
    "\n",
    "ニューラルネットワークは、画像認識、音声認識、自然言語処理など、さまざまな分野で高い性能を発揮しており、現代のAI技術の中核を担っています。\n",
    "\n",
    "## アヤメのデータセットと分類タスクについて\n",
    "\n",
    "アヤメのデータセット（Iris Dataset）は、機械学習で広く使われている有名なデータセットです。\n",
    "\n",
    "このデータセットには、3種類のアヤメの花（Setosa, Versicolor, Virginica）について、それぞれ50個体ずつの計150サンプルが含まれています。各サンプルには以下の4つの特徴量が記録されています。\n",
    "\n",
    "- がく片の長さ（sepal length）\n",
    "- がく片の幅（sepal width）\n",
    "- 花弁の長さ（petal length）\n",
    "- 花弁の幅（petal width）\n",
    "\n",
    "今回の分類タスクでは、これらの特徴量からアヤメの種類を予測するモデルをニューラルネットワークで作成します。\n",
    "\n",
    "## 本講義の目的\n",
    "\n",
    "本講義の目的は、以下の3点です：\n",
    "\n",
    "1. **ニューラルネットワークの基本的な構造と動作の理解**  \n",
    "   入力層、隠れ層、出力層の役割や、活性化関数、損失関数、重みの学習について学びます。\n",
    "\n",
    "2. **分類問題におけるニューラルネットワークの適用方法を学ぶ**  \n",
    "   データの前処理、モデルの構築、学習、評価の一連の流れを体験します。\n",
    "\n",
    "3. **実装を通じて、モデルのパラメータやハイパーパラメータが結果に与える影響を理解する**  \n",
    "   学習率、エポック数、層の数などを変えて結果を比較し、モデル設計の感覚を養います。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 必要なパッケージのインストール\n",
    "\n",
    "本講義で使うパッケージは以下のとおりです：\n",
    "\n",
    "- `keras` : neural networkモデルを作成できるパッケージ  \n",
    "- `pandas` : テーブルデータを操作するパッケージ  \n",
    "- `seaborn` : 統計的なデータ可視化に特化したライブラリ  \n",
    "- `matplotlib` : グラフや図を描画するための基本ライブラリ  \n",
    "- `numpy` : 数値計算を効率的に行うためのパッケージ  \n",
    "- `scikit-learn`（`sklearn`）: 機械学習のためのライブラリ  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo apt install -y graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow keras pandas seaborn matplotlib \"numpy==1.24.4\" pandas scikit-learn tensorboard lightgbm graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e97115b6-d0ee-44d7-b8cb-fbb28713e9c9",
    "_execution_state": "idle",
    "_uuid": "19970a38afe99ae032e4fe897a1cdadbc1445c5e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries \n",
    "import keras #library for neural network\n",
    "import pandas as pd #loading data in table form  \n",
    "import seaborn as sns #visualisation \n",
    "import matplotlib.pyplot as plt #visualisation\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import normalize #machine learning algorithm library\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7baa3b65-788f-419b-8981-eaeb21e71e70",
    "_execution_state": "idle",
    "_uuid": "2514ae5b171fa72efc6e0ef4848d45abed4feec3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Irisのデータはscikit-learnで用意されています。\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "data['target'] = iris.target # label\n",
    "\n",
    "print(data[\"target\"].unique())\n",
    "print(iris.target_names)\n",
    "# 実務でCSVなどからデータを取得する場合、以下のように読み込みます\n",
    "# data=pd.read_csv(\"../input/Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの確認\n",
    "\n",
    "ロードしたデータの中身を見てみましょう。\n",
    "- head: データの先頭数行を見ることができます。\n",
    "- describe: 統計情報を見ることができます\n",
    "- info: データのメタ情報を見ることができます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"10 first samples of the dataset:\", data.head(10), \"\\n\")\n",
    "print(\"10 last samples of the dataset:\", data.tail(10), \"\\n\")\n",
    "print(\"Describing the data: \\n\", data.describe(), \"\\n\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e81130c-0732-46d5-86ae-c476d89a2ff7",
    "_uuid": "8beee9188c5790a1bab4d265c62e4b087fb91108"
   },
   "source": [
    "## データセットのビジュアライゼーション (探索的データ解析; EDA)\n",
    "\n",
    "機械学習を始める前に、まずはデータをより理解する必要があります。\n",
    "データサイエンティストは、モデルの実装よりもデータへの理解やデータ整形に時間を必要とすることが一般的です。\n",
    "\n",
    "以下のコーディングは、データをより理解するためのデータセットの視覚化です。アイリスのすべての種が、予測すべき異なる領域に分別できることがわかります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6befa766-9ee0-431e-b47d-5ed5e687e4d1",
    "_uuid": "497e20251bd73940e6ec4791ff01d447515ceb6d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# JupyterLab で実行する際は、この行を書くことで描画できるようになります。\n",
    "%matplotlib inline\n",
    "sns.pairplot(data, hue=\"target\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f1f98d3-016b-4d4d-bf4a-4815709a32ce",
    "_uuid": "40540e69f5dda9908c28d8adf5314e381d346099"
   },
   "source": [
    "### データの前処理\n",
    "\n",
    "データへの理解が進んだところで、モデルに入力するためにデータ整形を行います。\n",
    "\n",
    "今回はシャッフルするだけですが、他にも色々な処理を入れることが一般的です\n",
    "- カテゴリ（label）を数値に変換する\n",
    "- 擬似的にデータにノイズをかけて、データ拡張\n",
    "- データの標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c3f20d0-2e81-4960-80e2-279975a427e7",
    "_uuid": "1d6426dc4b82c1a0cdde44bdd39e36d5068b0a9d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=data.iloc[np.random.default_rng(seed=42).permutation(len(data))]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96c0b24a-ef1d-44e7-872f-ce9065f4517f",
    "_uuid": "e9c7531857c15f4bcae71d1792e6ea2bddaf89c2"
   },
   "source": [
    "### データの分割\n",
    "説明変数(x)と目的変数(y)に分割します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b622856d-5fb1-458b-b395-bde6fbcc456a",
    "_uuid": "2bd511b4c77f6bcb831b620f237256ecbe966a29",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X=data.iloc[:,:4].values\n",
    "y=data.iloc[:,4:].values\n",
    "\n",
    "print(\"Shape of X\",X.shape)\n",
    "print(\"Shape of y\",y.shape)\n",
    "print(\"Examples of X\\n\",X[:3])\n",
    "print(\"Examples of y\\n\",y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "962d57b5-aa84-4170-a1a9-adb5e9eceb2a",
    "_uuid": "e6c5feb4fb4fefedcd03fd691ec6fe6370ef7dfb"
   },
   "source": [
    "### Normalization\n",
    "\n",
    "以上から、最初のデータセットの特徴は、Sepal Lengthが5.9cm、Sepal Widthが3.2cm、Petal Lengthが4.8cm、Petal Widthが1.8cmであることがわかります。\n",
    "しかし、カラム間で値のとりうる範囲は異なる可能性があるため、精度を維持するためには、各データセットの特徴を0-1の範囲に正規化して処理する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "366e4a6e-82f1-4723-8e9f-d4c47e9a9cee",
    "_uuid": "c11d1eed202d7b2d79c256364de928777d982178",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_normalized=normalize(X,axis=0)\n",
    "print(\"Examples of X_normalised\\n\",X_normalized[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習セットと検証セットの分割\n",
    "\n",
    "\n",
    "\n",
    "データセットを学習(training)セットと、モデルの汎化性を測るための検証(validation)セットに分割します。\n",
    "\n",
    "ここで注意が必要なのが、train dataの中に、validation setの情報が入っていないことです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9dced524-d0c2-486e-8654-dfaee41ce3f5",
    "_uuid": "0eb1b785254354dddab0642cac16ebff9bb824bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating train,test and validation data\n",
    "'''\n",
    "80% -- train data\n",
    "10% -- valid data\n",
    "10% -- test data\n",
    "'''\n",
    "total_length=len(data)\n",
    "train_length=int(0.8*total_length)\n",
    "valid_length=int(0.1*total_length)\n",
    "test_length=int(0.1*total_length)\n",
    "\n",
    "X_train=X_normalized[:train_length]\n",
    "X_valid=X_normalized[train_length:train_length+valid_length]\n",
    "X_test=X_normalized[train_length+valid_length:]\n",
    "y_train=y[:train_length]\n",
    "y_valid=y[train_length:train_length+valid_length]\n",
    "y_test=y[train_length+valid_length:]\n",
    "\n",
    "print(\"Length of train set x:\",X_train.shape[0],\"y:\",y_train.shape[0])\n",
    "print(\"Length of valid set x:\",X_valid.shape[0],\"y:\",y_valid.shape[0])\n",
    "print(\"Length of test set x:\",X_test.shape[0],\"y:\",y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "955494c5-3547-4f16-8cd4-c45fc4736c66",
    "_uuid": "e847b196b3d79e1aa11b1e6338a3300c6320cd5e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Neural network module\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense,Activation,Dropout \n",
    "from tensorflow.keras.layers import BatchNormalization \n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "82eae2e6-7369-4953-be41-725496a0c503",
    "_uuid": "4e43122514268433d653bf77e7b4ccbb8cd6e765",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Change the label to one hot vector\n",
    "'''\n",
    "[0]--->[1 0 0]\n",
    "[1]--->[0 1 0]\n",
    "[2]--->[0 0 1]\n",
    "'''\n",
    "y_train=utils.to_categorical(y_train,num_classes=3)\n",
    "y_valid=utils.to_categorical(y_valid,num_classes=3)\n",
    "y_test=utils.to_categorical(y_test,num_classes=3)\n",
    "print(\"Shape of y_train\",y_train.shape)\n",
    "print(\"Shape of y_valid\",y_valid.shape)\n",
    "print(\"Shape of y_test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル定義 : Neural Network\n",
    "いよいよモデルを定義していきます。\n",
    "\n",
    "これから作るモデル構造はざっくりと以下の通りです。\n",
    "![nn-model](./nn-model.png)\n",
    "\n",
    "### Dense\n",
    "すべての入力がすべての出力ノードに接続される層。主に特徴の重みづけ・学習を行います\n",
    "例えば入力が5次元、出力が3次元の場合は以下のようなモデル構造になります\n",
    "\n",
    "![dense](./dense.png)\n",
    "\n",
    "### Drop out\n",
    "→ ニューロンをランダムに一定割合「無効化」して、過学習（overfitting）を防ぐための正則化手法です\n",
    "\n",
    "以下の図は Dropout 40%でDense3からの入力をDropoutしている様子です。\n",
    "Dropoutによってモデルの一部を欠損させて学習させることで、モデルが頑健になります\n",
    "\n",
    "![dropout](dropout.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b39b3da2-db3e-4fac-a9a2-625142f4c2dc",
    "_uuid": "5e05d8f7e756fb0fe25820ba0fe7c6b15fdc8d1e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(1000,input_dim=4,activation='relu'))\n",
    "model.add(Dense(500,activation='relu'))\n",
    "model.add(Dense(300,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 小さいLearning Rateを設定（例: 0.0001）\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1000, activation='relu', input_shape=(4,)),\n",
    "    Dense(500, activation='relu'),\n",
    "    Dense(300, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e90b8a7-842d-457e-8615-c732425c8e70",
    "_uuid": "a308c53350dbaab53abe34c77aa7bceac39358eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9f9d7219-7a2d-405c-8e6c-c3097e5e7e06",
    "_uuid": "0b5ea6209168703be0e88e9a4b4a119eb21e896d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid,y_valid),\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習結果確認\n",
    "\n",
    "tensorboardを使うと、学習曲線と呼ばれる学習過程でのモデルのlossやaccuracyの遷移を確認することができます。\n",
    "\n",
    "学習曲線を見ることで、学習がうまくいったか、過学習していないかを観察します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networkモデルの評価\n",
    "\n",
    "保持していた `test` データセットを使ってモデルの精度を検証してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4af03573-a9a6-4cc8-afab-d098659a38b0",
    "_uuid": "7f296969c0072a6be7417c5d020de83e24bd0084",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction=model.predict(X_test)\n",
    "length=len(prediction)\n",
    "y_label=np.argmax(y_test,axis=1)\n",
    "predict_label=np.argmax(prediction,axis=1)\n",
    "\n",
    "accuracy=np.sum(y_label==predict_label)/length * 100 \n",
    "print(\"Accuracy of the dataset\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1267f5c-44ef-4140-b014-b94e34e8f9b5",
    "_uuid": "878d893cf9d4ee8ff9272e93757fdfb8f277790d"
   },
   "source": [
    "このデータセットでは、100%の精度が達成されました。\n",
    "\n",
    "Neural Networkは学習によって、テーブルデータからアヤメの種類を分類することができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "52532bef-70ad-4500-905d-a63ac20da4c7",
    "_uuid": "e55761033bce04b6026b94b1ffc8512b36ec3419"
   },
   "source": [
    "# EX：過学習させてみよう\n",
    "\n",
    "モデルのパラメータ数が大きくなると、モデルが過学習を起こしやすくなります。\n",
    "\n",
    "例えば、Dnese層のoutputの次元数を数倍にしてみて学習してみましょう。\n",
    "例えば、以下のようなグラフになれば成功です\n",
    "![overfitting](./over-fitting.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル定義 : 勾配ブースティング決定木 (GBDT)\n",
    "\n",
    "今度は、Neural Networkを使わずに、テーブルデータによく用いられる勾配ブースティング木を使って学習してみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ整形\n",
    "\n",
    "LightGBMはカテゴリの出力を勝手に内部でワンホットベクトルに直してくれるため、ワンホット→ラベルに直します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Change the label to one hot vector\n",
    "'''\n",
    "[1 0 0]--->[0]\n",
    "[0 1 0]--->[1]\n",
    "[0 0 1]--->[2]\n",
    "'''\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_valid = np.argmax(y_valid, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_valid:\", y_valid.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LightGBM用データセットに変換（バリデーションあり）\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの学習\n",
    "LightGBMモデルの定義を行います。\n",
    "\n",
    "## ハイパーパラメータの意味\n",
    "objective: モデルの目的。ここでは多クラス分類（multiclass）\n",
    "\n",
    "num_class: クラスの数。必ず multiclass とセットで指定します\n",
    "\n",
    "metric: 損失関数の種類。multi_logloss は一般的な多クラス分類に適した指標\n",
    "\n",
    "verbosity: ログの出力レベル。-1 にすることでログを抑制しています\n",
    "\n",
    "## コールバック関数の説明\n",
    "early_stopping: 検証データでのスコアが一定回数（ここでは10回）改善しないと、学習を途中で終了します（過学習防止）。\n",
    "\n",
    "log_evaluation: 指定された間隔（ここでは10回ごと）で評価結果を出力します。\n",
    "\n",
    "record_evaluation: 学習中のスコア（lossなど）を evals に保存し、後から学習曲線を可視化できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定（必要に応じてチューニング可）\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,  # アヤメは3クラス分類\n",
    "    'metric': 'multi_logloss',\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# 学習\n",
    "evals = {} # 学習曲線を保存する\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_valid],\n",
    "    num_boost_round=100,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=10, verbose=True),\n",
    "        lgb.log_evaluation(10),\n",
    "        lgb.record_evaluation(evals),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習曲線の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = lgb.plot_metric(evals)\n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価\n",
    "\n",
    "モデルの精度評価を行います\n",
    "100%で推論できていることがわかると思います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# テストデータで予測（確率 → クラス）\n",
    "y_pred_proba = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "# 評価\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=[\"setosa\", \"versicolor\", \"virginica\"])\n",
    "\n",
    "print(f\"✅ Accuracy: {acc*100:.4f} %\")\n",
    "print(\"📋 Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの分析\n",
    "\n",
    "木構造の確認と、特徴量の重要度を見てみましょう。\n",
    "\n",
    "## 木構造\n",
    "\n",
    "勾配ブースティング決定木によって学習された決定木モデルの１つをみてみましょう。\n",
    "小さい木構造で十分に推論が難しそうと感じるかもしれませんが、勾配ブースティングによるアンサンブル学習により、より高い精度を出すことが可能になっています\n",
    "\n",
    "## 特徴量の重要度\n",
    "\n",
    "どの特徴量が出力に寄与したかを見ることができます。下の図をみると、column2, column3がより重要となっており、`petal length (cm)` `petal width (cm)` の値が重要ということがわかります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgb.create_tree_digraph(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(model, importance_type=\"split\", figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
